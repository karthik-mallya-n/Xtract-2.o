#!/usr/bin/env python3
"""
Comprehensive verification that ALL rows from uploaded dataset are used in training
"""

import os
import pandas as pd
import json
from sklearn.model_selection import train_test_split

def verify_full_dataset_usage():
    """Verify that the training process uses every single row from the dataset"""
    
    print("ğŸ” COMPREHENSIVE DATASET USAGE VERIFICATION")
    print("=" * 80)
    
    # Find the most recent Walmart dataset upload
    uploads_dir = "uploads"
    if not os.path.exists(uploads_dir):
        print("âŒ No uploads directory found")
        return
        
    # Get all CSV files in uploads
    csv_files = [f for f in os.listdir(uploads_dir) if f.endswith('.csv')]
    if not csv_files:
        print("âŒ No CSV files found in uploads")
        return
    
    # Use the most recent CSV (or find Walmart-like data)
    csv_files.sort(reverse=True)
    dataset_path = os.path.join(uploads_dir, csv_files[0])
    
    print(f"ğŸ“‚ Using dataset: {dataset_path}")
    
    # Step 1: Load the ORIGINAL dataset
    print(f"\n1ï¸âƒ£ LOADING ORIGINAL DATASET")
    df_original = pd.read_csv(dataset_path)
    print(f"   ğŸ“Š Original dataset shape: {df_original.shape}")
    print(f"   ğŸ“Š Total rows: {len(df_original)}")
    print(f"   ğŸ“Š Columns: {list(df_original.columns)}")
    
    # Step 2: Simulate the EXACT same preprocessing as training
    print(f"\n2ï¸âƒ£ SIMULATING TRAINING PREPROCESSING")
    
    # Auto-detect target column (same logic as training)
    target_column = df_original.columns[-1]  # Last column as target
    print(f"   ğŸ¯ Target column: {target_column}")
    
    # Split features and target (EXACT same as training code)
    y = df_original[target_column]
    X = df_original.drop(columns=[target_column])
    
    print(f"   âœ… Features (X) shape: {X.shape}")
    print(f"   âœ… Target (y) shape: {y.shape}")
    print(f"   ğŸ” X + y total rows: {len(X)} + {len(y)} = {len(X) + len(y)}")
    print(f"   âœ… Matches original: {len(X) == len(df_original) and len(y) == len(df_original)}")
    
    # Step 3: Check for any data filtering/cleaning
    print(f"\n3ï¸âƒ£ CHECKING FOR DATA FILTERING")
    
    # Check for missing values
    missing_X = X.isnull().sum().sum()
    missing_y = y.isnull().sum()
    print(f"   ğŸ“Š Missing values in X: {missing_X}")
    print(f"   ğŸ“Š Missing values in y: {missing_y}")
    
    # In scikit-learn pipeline, missing values are handled by imputation, not removal
    print(f"   âœ… Missing values are IMPUTED (filled), not dropped")
    print(f"   âœ… All {len(df_original)} rows will be used")
    
    # Step 4: Simulate train/test split (EXACT same as training code)
    print(f"\n4ï¸âƒ£ SIMULATING TRAIN/TEST SPLIT")
    
    # Determine if classification for stratification
    unique_targets = y.nunique()
    target_dtype = y.dtype
    is_classification = unique_targets <= 20 and target_dtype in ['int64', 'int32', 'object', 'bool', 'category']
    
    stratify = y if is_classification else None
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=stratify
    )
    
    print(f"   ğŸ“Š Training set: {X_train.shape}")
    print(f"   ğŸ“Š Test set: {X_test.shape}")
    print(f"   ğŸ“Š Total samples: {len(X_train)} + {len(X_test)} = {len(X_train) + len(X_test)}")
    print(f"   âœ… Equals original: {len(X_train) + len(X_test) == len(df_original)}")
    
    # Step 5: Verify no rows are lost
    print(f"\n5ï¸âƒ£ FINAL VERIFICATION")
    total_used = len(X_train) + len(X_test)
    percentage_used = (total_used / len(df_original)) * 100
    
    print(f"   ğŸ“Š Original dataset: {len(df_original)} rows")
    print(f"   ğŸ“Š Used in training: {total_used} rows")
    print(f"   ğŸ“Š Percentage used: {percentage_used:.1f}%")
    print(f"   ğŸ“Š Rows lost/skipped: {len(df_original) - total_used}")
    
    if total_used == len(df_original):
        print(f"   âœ… SUCCESS: ALL {len(df_original)} ROWS ARE USED IN TRAINING!")
        print(f"   âœ… NO ROWS ARE FILTERED, SKIPPED, OR LOST!")
    else:
        print(f"   âŒ ERROR: {len(df_original) - total_used} rows are missing!")
    
    # Step 6: Check actual model metadata
    print(f"\n6ï¸âƒ£ CHECKING TRAINED MODEL METADATA")
    
    # Find most recent model
    models_dir = "models"
    if os.path.exists(models_dir):
        model_folders = [f for f in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, f))]
        if model_folders:
            # Get most recent timestamped model
            timestamped_models = [f for f in model_folders if '_20' in f]
            if timestamped_models:
                timestamped_models.sort(reverse=True)
                recent_model = timestamped_models[0]
                
                metadata_path = os.path.join(models_dir, recent_model, 'metadata.json')
                if os.path.exists(metadata_path):
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    
                    print(f"   ğŸ“‚ Recent model: {recent_model}")
                    print(f"   ğŸ“Š Features in model: {len(metadata.get('feature_names', []))}")
                    print(f"   ğŸ“Š Features list: {metadata.get('feature_names', [])}")
                    print(f"   ğŸ¯ Target: {metadata.get('target_column', 'N/A')}")
                    print(f"   ğŸ“ˆ Problem type: {metadata.get('problem_type', 'N/A')}")
                    
                    # Compare features
                    model_features = set(metadata.get('feature_names', []))
                    dataset_features = set(X.columns)
                    
                    if model_features == dataset_features:
                        print(f"   âœ… Model features EXACTLY match dataset features")
                    else:
                        print(f"   âš ï¸  Feature mismatch detected")
                        print(f"   Dataset: {dataset_features}")
                        print(f"   Model: {model_features}")
    
    print(f"\nğŸ‰ VERIFICATION COMPLETE!")
    return total_used == len(df_original)

if __name__ == "__main__":
    verify_full_dataset_usage()